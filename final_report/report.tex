%
% File naaclhlt2013.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx} % To include figures
\usepackage[titletoc]{appendix}

\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Automatic Reviewer: \\Predicting Whether a Paper will be Accepted or Rejected}

\author{Hong Wang\\
	    Department of Computer Science\\
	    University of Illinois at Chicago\\
	    Chicago, IL 60607, USA\\
	    {\tt hwang207@uic.edu}
	  \And
	Shuyang Lin\\
  	 Department of Computer Science\\
	    University of Illinois at Chicago\\
	    Chicago, IL 60607, USA\\
	  {\tt slin38@uic.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this work, we focus on the novel problem of predicting whether a paper will be accepted or rejected by a conference.
We proposed a framework for this predicting problem that consider the metadata of the paper as well as the context of the paper.
We evaluate the proposed approach using the ACL conference data set.
Empirical evluation shows that the proposed approach can outperform baseline by up to 10\% for the predicting task.

\end{abstract}

\section{Introduction}
After submitted a paper to a conference, you, as the author of the paper, usually are most interested in whether your paper will be accepted or not. However, it is the decision made by the reviewers of conference. A question arises here is that ``can we predict the result?'' or more specifically, ``can we find the criteria of a certain conference?'' In this paper, we introduce Automatic Reviewer, which may give you the answers, even before you submit the paper to the conference. 

This paper studies the following two problems: 1. What statistical features of papers are correlated to the decision of the conference? 2. To what extent can an automatic program predict whether a paper will be accepted or not?
To the best of our knowledge, we are the first to study these problems in the NLP area.

We proposed a model for solving this problem.
The model first parses the paper and then extract useful features from the paper.
The features considered by the model is various, which range from paper metadata to paper topics. 
After generating the features, 
we use supervised learning algorithm (SVM) to learn our system model, 
and evaluate it on the real data set.

Through empirical study, 
we find that the proposed approach can outperform all-positive baseline by up to 10\%,
and the topic of the paper, the metadata, and the complexity of sentences in the paper are three most useful predictors for the problem.



\section{Related Work}

Predicting has been studied in Machine Learning/Data Mining area for many years \cite{Bing}. Great successes have been made in areas range from e-commerce to economics by applying such predicting techniques. With the growing popularity of social networking, researchers are now trying to predict the trend by analyzing the topics of posts in the social networking \cite{Twitter}. Topic models of document like PLSI(Probabilistic Latent Semantic Indexing) and LDA (Latent Dirichlet Allocation) has been studied in \cite{Hofmann,Blei}  respectively.However, as far as we know, no similar study has been done in predicting conference paper acceptance. The only related study focus on the visual structure of conference papers \cite{Carven}. 
% [1] Bing Liu (2011), Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data, Second Edition. Springer
% [2] Predicting what topics will trend on Twitter, A new algorithm predicts which Twitter topics will trend hours in advance and offers a new technique for analyzing data that fluctuate over time. 
% [3] Hofmann, Thomas. Probabilistic Latent Semantic Indexing. Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999.
% [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, 2003. ISSN 1532-4435.
% [5] Paper Gestalt, Carven von Bearnensquash, http://vision.ucsd.edu/sites/default/files/gestalt.pdf













\section{Data Set}
Currently, we focus on ACL conference.
The full papers accepted by ACL are used as positive samples. 
As we do not have access to the real rejected papers, 
we use the papers in the joint workshops with ACL as the negative samples.
We made an assumption that the workshop papers are the papers rejected by the main conference.
The assumption is obviously not true for every ACL workshop paper.
But it still make sense for two reasons:
(1) Usually, workshop paper are not of as good quality as the main conference paper. 
(2) If a paper is rejected by the main conference, the authors will be suggested to submit the paper to the workshops come along with the conference.


We collect the papers which were published in ACL conference and joint workshops in year 2007, 2010, and 2012. 
They are the latest three ACL conferences that are not a joint conference.
We do not want to consider joint conference, because we want to focus on ACL conference alone.
After collecting the data, we filter out the workshop paper that is less than 8 pages, since the full papers accepted by ACL conference usually have 9 or 10 pages.
The statistics of data set is listed in Table \ref{tab:statistics}.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
Year & Positive &	Negative & Total\\ \hline 
2007 & 99  &	179 & 278\\ \hline	
2010 	& 160 &  144 & 304	\\ \hline
2012 	& 111 & 79 & 190 \\ \hline
Total & 370 & 402 & 772 \\\hline	
\end{tabular}
\end{center}
\caption{The Statistics of Data Set}
\label{tab:statistics}
\end{table}

\section{Approaches}
\subsection{Outline}
%Papers are first parsed into structured data (Java objects). Metadata from the paper are used directly as features. 
%Then the parsed papers are processed by several feature extractors. Finally, the feature vectors are sent to Support 
%Vector Machines (SVM) for model learning and evaluation.

\begin{figure*}[htb]
  \centering
	 	 \includegraphics[width=0.9\textwidth]{materials/architecture.eps}
		\caption{The architecture of the proposed approach}
		\label{fig:architecture}
\end{figure*}
In this study, we first downloaded all the papers from ACL web-site. Then we send them to Automatic Reviewer. There are three main components in Automatic Reviewer: 1. paper parser; 2. feature extractors; and 3. model and evaluation component. 
The architecture is showed in Figure \ref{fig:architecture}.

Papers are first parsed into structured data (Java objects). Metadata from the paper are used directly as features. Then the parsed papers are processed by several feature extractors. Finally, the feature vectors are sent to Support Vector Machines (SVM) for model learning and evaluation.




\subsection{Features}
We use several features: some of them are directly extracted from the paper; 
the others are generated by using more sophisticated techniques.

\begin{itemize} 
\item {\bf Metadata}
Metadata are those information extracted from paper itself, like the number of pages, total numbers of tables/formulas/figures, number of tables/formulas/figures per page, max number of tables/formulas/figures per page. In our current experiment, we only use number of tables/formulas/figures per page as features.

\item {\bf 	Author ranking}
We extracted top 2000 authors in ``Natural Language \& Speech'' area from Microsoft Academic Search \footnote{http://academic.research.microsoft.com/RankList?entitytype =2\&topDomainID=2\&subDomainID=9}, and use this extracted list as the authors' ranking. If author doesn't appear in the list, the rank will be zero.

\item {\bf 	Popular techniques mentioned in the paper}
We also manually created a list of popular techniques which may be used in the NLP research. The terms of techniques and its synonyms, abbreviations are group together. We check each technique group in the paper, to see if it has been mentioned or not.

\item {\bf Words in the title (TF-IDF Score)}
Based on the assumption that novel ideas are favored in the conference, and papers with novel ideas may have titles that contain infrequent terms which are more attractive to the reviewers, we count the TF-IDF score for the title terms, and use the score as one of the features.

\item {\bf Topic – LDA (Latent Dirichlet Allocation )}
LDA is used to extract topics from all the papers. In our experiment, top 20 topics are extracted, and the probability distribution vector of topics is used as one feature. 

\item {\bf 	Sentence complexity}
All the content sentences within papers are parsed into phrase structure trees by using Stanford Parser \footnote{:http://nlp.stanford.edu/software/lex-parser.shtml}. The depth of tree is used as complexity of the corresponding sentence.  
For each paper, we count the number of sentences with each possible value of complexity,
and use the frequency of all the possible values of complexity as a feature vector.
\end{itemize}




\subsection{Paper parser}

There are three sub-components in the paper parser: Apache Tika \footnote{Apache Tika, http://tika.apache.org/}, Metadata Extractor, and Title \& Author Extractor. 

The original papers downloaded from ACL web-site are first sent to Apache Tika which is a PDF file parser. It parses PDF file into HTML-like structured data. The paper is parsed into several \textless page\textgreater tags which contain \textless p\textgreater tags that denote the raw paragraphs in the original paper. 

Because the parsing is not very accurate, the raw paragraphs contain all the texts in the paper, including the page foot, like ``Proceedings of …'', ``BioNLP 20...''. After all these noise paragraphs are discarded by regular expression matching, the first raw paragraph is considered as the candidate title. The paragraph starts with ``abstract'' is the abstract paragraph. If there's no paragraph starts with ``abstract'', the first paragraph which contains more than 300 English letters is considered as the abstract paragraph. 
We consider all the paragraphs between title and abstract as raw author info which contains author names, affiliations, and email addresses. All paragraphs after the key-word ``reference'' or ``references'' are considered as paper references. The other paragraphs between abstract and references are considered as content paragraphs. 

Content paragraphs are then sent to Metadata Extractor. Metadata Extractor uses regular expression \verb- "\^(fig\.|figure)\s*\d+"- and  \verb-"\^(tab\.|table)\s*\d+"- to find the figures and tables in each page. Continuous paragraphs which contain more than 30\% non- alphabet English letters are considered as one formula. These approaches may not be very accurate. However, since both training and testing papers are parsed based on this same rule, we think the inaccuracy is acceptable. Using the methods above, all the metadata are generated. 

Candidate title and raw author information are processed by Title \& Author Extractor. This extractor utilizes DBLP's web service API \footnote{http://www.dblp.org/search/api/?q=[TITLE], where "[TITLE]" is the paper title to search.}. This API supports fuzzy query search. We send the raw title to the API and retrieve the most reliable one as the real title for the paper. If a paper does not exist in DBLP, the raw title is accepted. Together with the title, author names are also returned from DBLP web API. The extractor uses regular expression to match the email addresses within raw author info, the text between the author name and matched email addresses are considered as the author's affiliation. Currently, only title and author names are further used in our experiment. 

Through the paper parser, the PDF paper is parsed into structured Java ``Paper'' object. ``Paper'' contains one ``Metadata'', several ``Author'' objects, and all sentences in each paragraph. The corresponding UML is attached in the Appendix.


\subsection{Feature Extractors}
Besides the metadata feature which directly comes from the parsed Metadata object, other features are generated from the following 4 feature extractors. 

\begin{itemize} 
\item {\bf Author ranker}
Author ranker extracts top 2000 authors in ``Natural Language \& Speech'' area from Microsoft Academic Search. Then the author names, affiliations and ranking are stored in a Lucene \footnote{Apache Lucene, http://lucene.apache.org/} index. Because abbreviation of names may appear in the paper, we send fuzzy query of author name to the Lucene index to retrieve the ranking information. Also an empirical matching score threshold is set in order to filter out those unlikely matching. 


\item {\bf Term analyzer}
Term analyzer tries to catch technical concepts from paper at term-level. Right now this analyzer mainly focuses on the popular technique terms mentioned in the paper. Unlike topics of paper, these technique terms only concentrate on technique aspects of the paper content. We manually created a list of technique terms and their synonyms and abbreviations. 
The term list is shown in Table \ref{tab:terms}.
Term analyzer uses this term list to generate a Boolean feature vector. Each value in the vector corresponds to the existence of each technique term.

\begin{table}[htb]
\begin{center}
\begin{tabular}{|c|p{5cm}|}
\hline 
Term & Forms\\ \hline 
LDA	& LDA; Latent Dirichlet allocation\\ \hline 
HMM	& HMM; HMMs; Hidden Markov model\\ \hline 
MaxEnt	&MaxEnt; maximum entropy\\ \hline 
MEMM &	MEMM; maximum-entropy Markov model; maximum entropy Markov model\\ \hline 
CRF	& CRF; CRFs; Conditional random fields\\ \hline 
NER	& NER; named-entity recognition; named entity recognition; entity identification; entity extraction\\ \hline 
SVM	& SVM; SVMs; support vector machine; support vector machine; support vector network; support vector networks\\ \hline 
LogisticRegression	& logistic regression \\ \hline 
LinearRegression	&linear regression\\ \hline 
LSI &	LSI; latent semantic indexing; SVD; singular value decomposition; LSA; latent semantic analysis\\ \hline 
KLdivergence &	KL-divergence; KLdivergence; Kullback-Leibler divergence; Kullback Leibler divergence; information gain\\ \hline 
MutualInformation&	mutual information;\\ \hline 
\end{tabular}
\end{center}
\caption{Popular techinique terms used by term analyzer}
\label{tab:terms}
\end{table}








\item	{\bf Topic analyzer}
We believe that during a period of time, certain topics are more interested in certain research area, and those papers studied such topics have more chances been accepted by the conference in that research area. In order to find out what topics are more favored in the conference, we use LDA (Latent Dirichlet allocation) to extract 20 topics from all the training data. The probability distribution of the topics of paper is used as a feature vector. 

\item	{\bf Sentence analyzer}
Because there is a limit in the number of pages of conference paper, we think the sentence would be more complicated if the paper contains more information, and such paper are more valuable than others. Based on this assumption, we use sentence analyzer to analyze the complexities of sentences in each paper. The paragraphs in paper are split into sentences first. And then these sentences are parsed by Stanford Parser. After we got the parsed structure tree from Stanford Parser, the depth of each tree is used as the measurement of sentence complexity. 

\end{itemize} 









\section{Results and Discussion}
We use the SVM with 10-fold cross-validation to evaluate our Automatic Reviewer system. The result of all positive predictions is used as the baseline. 


\subsection{Evaluation of Each Feature}
\label{sec:feature_eval}
As first part of our experiment, 
we evaluate the classifiers that learned using each single feature.
For the evaluation,
We conduct 10-fold cross-validation on the ACL 2012 data set,
and report the precision, recall and F-score for the classier using each single feature.

As shown in the Table \ref{tab:feature_eval},
LDA Topic feature and sentence complexity feature are two best predictors for this problem.
Each of them can construct a classifier that is better than the all-positive baseline.
Each of the metadata feature (number of pictures/formulas/tables on each page),
when is used alone, is a weak predictor.
But when we consider all the features from metadata together,
we get a classifier that is significantly outperforms the all-positive baseline.
Terms in title and popular techniques are two other weak predictors,
but they can still make a classifier with higher precision than the baseline,
which means when they are used together with other features, they can contribute to a better classifier.
The author ranking feature is the weakest predictor among all the features we consider.

The result shows that:
(1) The full papers accepted by ACL 2012 are statistically 
different from the workshop papers in the complexity of sentences and topic of the paper.
(2) The numbers of pictures/formulas/tables in the full papers accepted by ACL are larger
comparing with those of workshop papers.
(3) The ranking of authors of a paper does not have influence on whether the paper 
is an ACL conference full papers or not, 
which may result from the double-blind reviewing of ACL 2012. 

\begin{table}
\begin{center}
\begin{tabular}{|p{2cm}|c|c|c|}
\hline 
& \bf Precision & \bf Recall & \bf F-score \\ \hline
\#Pictures & 63.6 & 68.5 & 65.8 \\ \hline
\#Formulas	& 74.5	& 68.5	& 71.4\\ \hline
\#Tables	&70.0 &	80.2	& 74.8\\ \hline
Metadata	& 74.8	& 82.9	& 78.6\\ \hline
Title	& 65.2	& 91.0 &	76.0\\ \hline
LDA Topic &	69.9	&91.9&	79.4\\ \hline
Popular Techniques	& 67.0	& 47.7	&55.8\\ \hline
Author Ranking	& 57.0	& 76.6	& 65.4\\ \hline
Sentence Complexity	& 71.4	& 90.1 &	79.7\\ \hline
All Positive	& 58.4	& 100.0	& 73.7\\\hline
\end{tabular}
\end{center}
\caption{Evaluation of Classifiers Using Each Single Feature}
\label{tab:feature_eval}
\end{table}



\subsection{Evaluation on Different Data Sets}
In this part, we evaluate the proposed approach on different data sets.
We collect the ACL conference and workshop papers on the years 2012, 2010 and 2007,
and evaluate our approach by conducting cross-validation on each of the three data sets.

For each data set we compare the evaluation result of the classifier learned by all the features with the all-positive baseline.
Since metadata feature, LDA topic feature and sentence complexity feature are three most useful features, as discussed in Section \ref{sec:feature_eval}
, we also list the result of the classifier learned from each of the three features as well as the one learned from three of them. 
Table \ref{tab:2012}, \ref{tab:2010} and \ref{tab:2007} show the result of ACL 2012, 2010 and 2007 data sets respectively.



\begin{table}
\begin{center}
\begin{tabular}{|p{2cm}|c|c|c|}
\hline 
& \bf Precision & \bf Recall & \bf F-score \\ \hline
Metadata	& 74.8	& 82.9	& 78.6\\ \hline
LDA Topic &	69.9	&91.9&	79.4\\ \hline
Sentence Complexity	& 71.4	& 90.1 &	79.7\\ \hline
Metadata \&LDA\& Complexity	& 79.8	& 92.8 & \bf 85.8\\ \hline
All features	& 80.5	& 85.6	& 83.0\\ \hline
All Positive	& 58.4	& 100.0	& 73.7\\\hline
\end{tabular}
\end{center}
\caption{Evaluation on ACL 2012 Data Set}
\label{tab:2012}
\end{table}


\begin{table}
\begin{center}
\begin{tabular}{|p{2cm}|c|c|c|}
\hline 
& \bf Precision & \bf Recall & \bf F-score \\ \hline
Metadata	&70.1&68.8&69.4\\ \hline
LDA Topic &65.4&66.3&65.8\\ \hline
Sentence Complexity	&65.3&60.0&62.5\\ \hline
Metadata \&LDA\& Complexity	&69.3&72.9&\bf 70.6\\ \hline
All features	&67.2&69.4&68.3\\ \hline
All Positive	&52.7& 100.0	& 68.9\\\hline
\end{tabular}
\end{center}
\caption{Evaluation on ACL 2010 Data Set}
\label{tab:2010}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|p{2cm}|c|c|c|}
\hline 
& \bf Precision & \bf Recall & \bf F-score \\ \hline
Metadata	& 35.2	& 31.3	& 33.2\\ \hline
LDA Topic &	59.2	& 58.6&	58.9\\ \hline
Sentence Complexity	& 65.3	& 60.0 &	\bf 62.5\\ \hline
Metadata \&LDA\& Complexity	& 42.7	& 41.4 & 42.1\\ \hline
All features	& 60.5	& 49.5	& 54.5 \\ \hline
All Positive	& 35.6	& 100.0	& 52.5\\ \hline
\end{tabular}
\end{center}
\caption{Evaluation on ACL 2007 Data Set}
\label{tab:2007}
\end{table}






As shown in these tables,
the proposed approach performs best on the ACL 2012 data set,
and can get slightly better performance than the baseline on the ACL 2010 and 2007 data set.

On the ACL 2012 data set, the features ``Metadata",``LDA topic" and ``Sentence Complexity" 
all have better F-score than the baseline.
When three of them considered together, the F-score is significantly better then the baseline.
The F-score of the classifier using all features is slightly lower than the three-best-features classifier,
but still outperforms the baseline by 10\%.

On the ACL 2010 data set, each of the three features is not as strong predictor as it is on the ACL 2012 data set.
But when we create a classifier use all of the three of them, 
the result is better than the baseline. 
But if we consider all the features together, 
the result classifier is not better than the baseline.

On the ACL 2007 data set, the sentence complexity feature alone is a good predictor.
It can make a classifier that is 10\% outperforms the all-positive baseline.
Other features are not as good as the sentence complexity feature.
When consider all the features together, the result is not as good as the one using only the sentence complexity feature,
but it is still better than the baseline.

As we have shown above, the results of evaluation are different on each data set, because features may work well on one data set, 
but not so well on another data set.
It is still not clear to us why features have different performance on different data set.
It may because of the criteria of reviewing gradually changes as time goes by, 
but may also be explained by the fact that each year the ACL conference has different set of workshops comes alone with it.





\subsection{Conclusion and Future Work} 
From the result of evaluation, we can see that the proposed model can predict whether a paper is accepted or not with good accuracy. 
The metadata, sentence complexity and the topic distribution learned by LDA model are the three best features.
This result proved our assumption that certain topics are more favored by certain conference,
and the complexity of the sentences also influence whether it will be accepted or not.
Also, tables number is the best of all three metadata, which indicates that the reviewers of paper are more willing to see the well structured result of research rather than plain text.







Features like popular techniques and author ranking do not work very good. 
This phenomena indicates that the quality of ACL conference is very high, the papers are thoroughly studied before accepting/rejecting, and the conference cares more about the content of paper rather than whether the techniques used are popular or not. 
Because the reviewing is blind, the author's ranking cannot influence the decision of the reviewers. 


In the future, we want to study how the topic/style of papers changes over the time. 
And we also want to extend our research to other conferences in other area.
We want to find out the main difference between the accepted papers in different conferences, and different areas. 
Currently, the negative samples are workshop papers which may not be representative. 
We are considering using papers come from lower-level conferences within the same research area as the negative samples. 





\begin{thebibliography}{}

\bibitem[\protect\citename{Liu}2011]{Bing}
B. Liu
\newblock 2011.
\newblock {\em Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data, Second Edition}
\newblock Springer.

\bibitem[\protect\citename{{Hardesty}}2012]{Twitter}
L. Hardesty
\newblock 2012.
\newblock {\em Predicting what topics will trend on Twitter}
\newblock http://web.mit.edu/newsoffice/2012/predicting-twitter-trending-topics-1101.html

\bibitem[\protect\citename{{Hofmann}}1999]{Hofmann}
T. Hofmann
{Probabilistic Latent Semantic Indexing}.
\newblock 1999.
\newblock {\em in Proceedings of the Twenty-Second Annual International SIGIR Conference}

\bibitem[\protect\citename{Blei}2003]{Blei}
D. M. Blei, A. Y. Ng, and M. I. Jordan. 
\newblock 2003.
\newblock {\em Latent dirichlet allocation},
\newblock {\em Journal of Machine Learning research}, 
3 , 993--1022 .

\bibitem[\protect\citename{Bearnensquash}1997]{Carven}
C. V. Bearnensquash\newblock 1997.
\newblock 2010.
\newblock {\em Paper Gestalt}.
\newblock http://vision.ucsd.edu/sites/default/files/gestalt.pdf

\end{thebibliography}




\appendix

\section{Additional materials}
Figure \ref{fig:uml} is the UML for the parsed Java Paper object.  



\begin{figure*}[htb]
  \centering
	 	 \includegraphics[width=0.7\textwidth]{materials/paper_uml.png}
		\caption{UML for the parsed Java Paper object}
		\label{fig:uml}
\end{figure*}





\section{Project Workload}
See the Figure \ref{fig:workload}.
\begin{itemize}
\item The blue parts (includes paper parser, major part of feature extracting) are done by Hong Wang.
\item	The red parts (includes data collecting, minor part of feature extracting, feature generating and classifier) are done by Shuyang Lin.
\item	This project report and previous presentation slides are done by both of them. 
\end{itemize}


\begin{figure*}[htb]
  \centering
	 	 \includegraphics[width=0.5\textwidth]{materials/workload.eps}
		\caption{Workload}
		\label{fig:workload}
\end{figure*}





\end{document}

\section{Introduction}

The following instructions are directed to authors of papers accepted
for publication in the NAACL HLT 2013 proceedings.  All authors are required
to adhere to these specifications. Authors are required to provide 
a Portable Document Format (PDF) version of
their papers.  The proceedings will be printed on US-Letter paper.
Authors from countries in which access to word-processing systems is
limited should contact the publication chairs as soon as possible.

\paragraph{What's new} This year, grayscale readability will be enforced for all accepted
papers (\S\ref{ssec:accessibility}).  Apart from this, the style files and camera-ready requirements
are unchanged from last year.

\section{General Instructions}

Manuscripts must be in two-column format.  Exceptions to the
two-column format include the title, as well as the 
authors' names and complete
addresses (only in the final version, not in the version submitted for review), 
which must be centered at the top of the first page (see
the guidelines in Subsection~\ref{ssec:first}), and any full-width
figures or tables.  Type single-spaced.  Do not number the pages.
Start all pages directly under the top margin.  See the guidelines
later regarding formatting the first page.

%% If the paper is produced by a printer, make sure that the quality
%% of the output is dark enough to photocopy well.  It may be necessary
%% to have your laser printer adjusted for this purpose.  Papers that are too
%% faint to reproduce well may not be included.

%% {\bf Do not print page numbers on the manuscript.}  Write them lightly
%% on the back of each page in the upper left corner along with the
%% (first) author's name.

The maximum length of a manuscript is eight (8) pages for the main
conference, printed single-sided, plus two (2) pages for references
(see Section~\ref{sec:length} for additional information on the
maximum number of pages).  Do not number the pages.

The review process is double-blind, so do not include any author information (names, addresses) when submitting a paper for review.  However, you should allocate space for the names and addresses so that they will fit in the final (accepted) version.  This is best done by either providing fake or blank names and addresses (as shown in this paper).

\subsection{Electronically-available resources}

NAACL HLT provides this description in \LaTeX2e{} ({\tt naaclhlt2013.tex}) and PDF
format ({\tt naaclhlt2013.pdf}), along with the \LaTeX2e{} style file used to
format it ({\tt naaclhlt2013.sty}) and an ACL bibliography style ({\tt naaclhlt2013.bst}).
These files are all available at
{\tt http://naacl2013.naacl.org}.  A Microsoft Word
template file ({\tt naaclhlt2013.dot}) is also available at the same URL. We
strongly recommend the use of these style files, which have been
appropriately tailored for the NAACL HLT 2013 proceedings.


\subsection{Format of Electronic Manuscript}
\label{sect:pdf}

For the production of the electronic manuscript you must use Adobe's
Portable Document Format (PDF). This format can be generated from
postscript files: on Unix systems, you can use {\tt ps2pdf} for this
purpose; under Microsoft Windows, you can use Adobe's Distiller, or
if you have cygwin installed, you can use {\tt dvipdf} or
{\tt ps2pdf}.  Note 
that some word processing programs generate PDF which may not include
all the necessary fonts (esp. tree diagrams, symbols). When you print
or create the PDF file, there is usually an option in your printer
setup to include none, all or just non-standard fonts.  Please make
sure that you select the option of including ALL the fonts.  {\em
  Before sending it, test your {\/\em PDF} by printing it from a
  computer different from the one where it was created}. Moreover,
some word processor may generate very large postscript/PDF files,
where each page is rendered as an image. Such images may reproduce
poorly.  In this case, try alternative ways to obtain the postscript
and/or PDF.  One way on some systems is to install a driver for a
postscript printer, send your document to the printer specifying
``Output to a file'', then convert the file to PDF.

For reasons of uniformity, Adobe's {\bf Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\begin{quote}
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\end{quote}
in the preamble.

Additionally, it is of utmost importance to specify the {\bf
  US-Letter format} (8.5in $\times$ 11in) when formatting the paper.
When working with {\tt dvips}, for instance, one should specify {\tt
  -t letter}.

Print-outs of the PDF file on US-Letter paper should be identical to the
hardcopy version.  If you cannot meet the above requirements about the
production of your electronic submission, please contact the
publication chairs above  as soon as possible.


\subsection{Layout}
\label{ssec:layout}

Format manuscripts two columns to a page, in the manner these
instructions are formatted. The exact dimensions for a page on US-letter
paper are:

\begin{itemize}
\item Left and right margins: 1 inch
\item Top margin: 1 inch
\item Bottom margin: 1 inch
\item Column width: 3.15 inches
\item Column height: 9 inches
\item Gap between columns: 0.2 inches
\end{itemize}

\noindent Papers should not be submitted on any other paper size. Exceptionally,
authors for whom it is \emph{impossible} to format on US-Letter paper,
may format for \emph{A4} paper. In this case, they should keep the \emph{top}
and \emph{left} margins as given above, use the same column width,
height and gap, and modify the bottom and right margins as necessary.
Note that the text will no longer be centered.

\subsection{The First Page}
\label{ssec:first}

Center the title, author's name(s) and affiliation(s) across both
columns (or, in the case of initial submission, space for the names). 
Do not use footnotes for affiliations.  Do not include the
paper ID number assigned during the submission process. 
Use the two-column format only when you begin the abstract.

{\bf Title}: Place the title centered at the top of the first page, in
a 15 point bold font.  (For a complete guide to font sizes and styles, see Table~\ref{font-table}.)
Long title should be typed on two lines without
a blank line intervening. Approximately, put the title at 1in from the
top of the page, followed by a blank line, then the author's names(s),
and the affiliation on the following line.  Do not use only initials
for given names (middle initials are allowed). Do not format surnames
in all capitals (e.g., ``Bangalore,'' not ``BANGALORE'').  The affiliation should
contain the author's complete address, and if possible an electronic
mail address. Leave about 0.75in between the affiliation and the body
of the first page.

{\bf Abstract}: Type the abstract at the beginning of the first
column.  The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.25in on each side.  Center the word {\bf Abstract} in a 12 point
bold font above the body of the abstract. The abstract should be a
concise summary of the general thesis and conclusions of the paper.
It should be no longer than 200 words.  The abstract text should be in 10 point font.

{\bf Text}: Begin typing the main body of the text immediately after
the abstract, observing the two-column format as shown in 
the present document.  Do not include page numbers.

{\bf Indent} when starting a new paragraph. For reasons of uniformity,
use Adobe's {\bf Times Roman} fonts, with 11 points for text and 
subsection headings, 12 points for section headings and 15 points for
the title.  If Times Roman is unavailable, use {\bf Computer Modern
  Roman} (\LaTeX2e{}'s default; see section \ref{sect:pdf} above).
Note that the latter is about 10\% less dense than Adobe's Times Roman
font.

\subsection{Sections}

{\bf Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals. 

{\bf Citations}: Citations within the text appear
in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
the text itself, as Gusfield~\shortcite{Gusfield:97}. In \LaTeX2e, the former is accomplished using
\verb|\cite| and the latter with \verb|\shortcite| or \verb|\newcite|.
Append lowercase letters to the year in cases of ambiguities.  
Treat double authors as in~\cite{Aho:72}, but write as 
in~\cite{Chandra:81} when more than two authors are involved. 
Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}.

\textbf{References}: Gather the full set of references together under
the heading {\bf References}; place the section before any Appendices,
unless they contain references. Arrange the references alphabetically
by first author, rather than by order of occurrence in the text.
Provide as complete a citation as possible, using a consistent format,
such as the one for {\em Computational Linguistics\/} or the one in the 
{\em Publication Manual of the American 
Psychological Association\/}~\cite{APA:83}.  Use of full names for
authors rather than initials is preferred.  A list of abbreviations
for common computer science journals can be found in the ACM 
{\em Computing Reviews\/}~\cite{ACM:83}.

The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
American Psychological Association format, allowing regular citations, 
short citations and multiple citations as described above.

{\bf Appendices}: Appendices, if any, directly follow the text and the
references (but see above).  Letter them in sequence and provide an
informative title: {\bf Appendix A. Title of Appendix}.

\textbf{Acknowledgment} sections should go as a last (unnumbered) section immediately
before the references.  

\subsection{Footnotes}

{\bf Footnotes}: Put footnotes at the bottom of the page. They may
be numbered or referred to by asterisks or other
symbols.\footnote{This is how a footnote should appear.} Footnotes
should be separated from the text by a line.\footnote{Note the
line separating the footnotes from the text.}  Footnotes should be in 9 point font.

\subsection{Graphics}

{\bf Illustrations}: Place figures, tables, and photographs in the
paper near where they are first discussed, rather than at the end, if
possible.  Wide illustrations may run across both columns and should be placed at
the top of a page. Color illustrations are discouraged, unless you have verified that 
they will be understandable when printed in black ink. 

\begin{table}
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
abstract text & 10 pt & \\
captions & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

{\bf Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and 
tables below the body, using 10 point text.  

\subsection{Accessibility}
\label{ssec:accessibility}

In an effort to accommodate the color-blind (as well as those printing to paper), grayscale
readability for all accepted papers will be enforced.  Color is not forbidden, but authors should
ensure that tables and figures do not rely solely on color to convey critical distinctions.

\section{Length of Submission}
\label{sec:length}

The NAACL HLT 2013 main conference accepts submissions of long papers
and short papers.  The maximum length of a long paper manuscript is
eight (8) pages of content and two (2) additional pages of references
\emph{only} (appendices count against the eight pages, not the
additional two pages).  The maximum length of a short paper manuscript
is four (4) pages and two (2) additional pages of references.  For
both long and short papers, all illustrations, references, and
appendices must be accommodated within these page limits, observing
the formatting instructions given in the present document.  Papers
that do not conform to the specified length and formatting
requirements are subject to be rejected without review.

\section{Double-blind review process}
\label{sec:blind}

As the reviewing will be blind, the paper must not include the authors' names and affiliations. Furthermore, self-references that reveal the author's identity, e.g., ``We previously showed (Smith, 1991) ...'' must be avoided. Instead, use citations such as ``Smith previously showed (Smith, 1991) ...'' Papers that do not conform to these requirements will be rejected without review. In addition, please do not post your submissions on the web until after the review process is complete.

\section*{Acknowledgments}

Do not number the acknowledgment section.

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Bearnensquash}1997]{Carven}
C. V. Bearnensquash\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
